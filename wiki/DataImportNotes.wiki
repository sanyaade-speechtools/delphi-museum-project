#summary I am gathering some ideas for better supporting import and cleaning of metadata from a CMS. Am starting from the PAHMA work taking TMS data.

== Introduction ==

Right now, the processing tools just take in a nice clean CSV file of metadata, sorted by objectID, and allowing multiple lines per object (since flattening a DB often does not come out clean and easy on a single line).

In addition, there is typically a fair amount of data cleansing that is required for a variety of reasons. In a perfect world, this would happen in the CMS before we ever got to it. In this universe, we often have to filter annotations that are not appropriate for public consumption, or would just be noise to the public user. We can describe regular expressions to recognize data to be elided. This is really only worth the work if there are some common phrases or annotations that are only for internal use.

Another issue arises as we often have to transform the data in some manner for public consumption. E.g., if your descriptions occasionally have price information or detailed location information (yes, this happens), you may have to obfuscate these values for public consumption. It is often not feasible to make these changes in the CMS data, and so this is just a part of the processing for exposure. This can be done with some regexp rules that are reusable across many deployments.

== Model Notes ==

Want to be able separate this into several steps:
  # Specify the query to the data source. We want this to be abstract, referencing the table, the column that keys to the object, and the columns to be pulled in as data. We could complicate things by supporting joins in here, or we could require that joins be done as views in the original DB, and then reference those. Perhaps we can support some text statements to set up the views as prep. This would be custom for each CMS/deployment, but we would concentrate the custom bits there.
  # Describe the mapping of DB columns to destination columns for data mining. Note that we may get multiple sources flowing into a given column for a given object, and so will need rules for how to handle this. We can create a new column just by giving the destination column a unique name. A reference to a column name previously described will replace or append content, using a rule:
    * We can replace previous values by specifying a "replace" rule. A variant on this would be a "replace-if-empty" rule, that would drop a new value if there is some value already there.
    * We can append to an existing column with an "append" rule. A variant on this may be needed to only append if the string is not already contained in the current value. This helps with funky data models that duplicate data.
  # Describe rules to discard entire rows. If there is a remark on an annotation that indicates a stale or otherwise useless annotation ("marked for delete", "obsolete", etc.) we can filter these in the query. The rule will reference a column and a pattern expression, and the effect is to discard that row (when the pattern is matched). _Implementation note: we can extend the query to filter these rows_
  # Describe rules to discard particular column values. Certain common notations may have administrative utility, but make little sense for public exposure. The rule will reference a column and a pattern expression. The effect is to elide the entire value for that column (when the pattern is matched), but to retain other columns for that row. _Implementation note: we can extend the query to filter these values, or we can do this processing on the result set._
  # Describe rules to modify strings within column values. This is useful for anonymizing names, obfuscating prices and storage locations, etc. The rule will reference a column plus a pattern expression and a replacement expression. _Implementation note: we can extend the query to filter these values, or we can do this processing on the result set. Probably much faster to do it on the result set than in MySQL._

Here's one possibility for an input syntax. Ideally this would be generated by a UI that browsed the data source and let you map things visually, adding cleaning and filtering rules with a form UI. A related UI would help build the column configuration for data mining, to establish which facets to mine from which columns, etc. SMOP...

{{{
<setup>CREATE VIEW foo AS SELECT a.id, b.val, c.note FROM a, b, c WHERE...</setup>
<sourceTable name="foo" keyColumn="id">
  <mapColumn sourceCol="val" destCol="actions" replaceRule="append" >
    <skipColValue pattern="priced">These are for accounting review.</skipColValue>
    <skipColValue pattern="moved*">These are not interesting.</skipColValue>
  </mapColumn>
  <mapColumn sourceCol="note" destCol="notes" replaceRule="append" >
    <skipColValue pattern="*Joe Brown says*">Remove attribution notes.</skipColValue>
    <skipColValue pattern="*updated on*">Remove tracking notes.</skipColValue>
    <replaceStr
       pattern="([$£€])?(\d{1,3},?(\d{3},?)*\d{3}(\.\d{0,2})?|\d{1,3}(\.\d{0,2})?|\.\d{1,2}?)" 
       replace="\1##.##">Obfuscate currency values. Handles: $1,000 £20.93 €0.50</replaceStr> 
 </mapColumn>
  <skipRow patternCol="note" pattern="*obsolete*">Ignore all values marked obsolete</skipRow>
</sourceTable>
}}}